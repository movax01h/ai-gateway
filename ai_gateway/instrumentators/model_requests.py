import time
from contextlib import contextmanager
from typing import Any, Optional

import structlog
from gitlab_cloud_connector import GitLabUnitPrimitive
from langchain_core.messages import BaseMessage
from langchain_core.messages.ai import UsageMetadata
from prometheus_client import Counter, Gauge, Histogram

from ai_gateway.api.feature_category import current_feature_category
from ai_gateway.config import ModelLimits
from ai_gateway.tracking.errors import log_exception
from lib.context import (
    METADATA_LABELS,
    LLMFinishReason,
    LlmOperations,
    TokenUsage,
    build_metadata_labels,
    client_capabilities,
    client_type,
    get_llm_operations,
    get_token_usage,
    gitlab_realm,
    gitlab_version,
    init_llm_operations,
    init_token_usage,
    language_server_version,
    llm_operations,
    token_usage,
)
from lib.internal_events.client import InternalEventsClient
from lib.internal_events.context import InternalEventAdditionalProperties

# Error type constants
ERROR_TYPE_NONE = "none"
ERROR_TYPE_PROMPT_TOO_LONG = "prompt_too_long"
ERROR_TYPE_OVERLOADED = "overloaded"
ERROR_TYPE_HTTP_400 = "http_400"
ERROR_TYPE_PERMISSION_ERROR = "permission_error"
ERROR_TYPE_SERVICE_UNAVAILABLE = "service_unavailable"
ERROR_TYPE_OTHER = "other"


METRIC_LABELS = ["model_engine", "model_name"]
INFERENCE_DETAILS = (
    METRIC_LABELS
    + METADATA_LABELS
    + [
        "error",
        "error_type",
        "streaming",
        "feature_category",
        "unit_primitive",
        "finish_reason",
    ]
)

INFERENCE_IN_FLIGHT_GAUGE = Gauge(
    "model_inferences_in_flight",
    "The number of in flight inferences running",
    METRIC_LABELS,
)

MAX_CONCURRENT_MODEL_INFERENCES = Gauge(
    "model_inferences_max_concurrent",
    "The maximum number of inferences we can run concurrently on a model",
    METRIC_LABELS,
)
MAX_MODEL_INPUT_TOKENS = Gauge(
    "model_max_input_tokens",
    "The per-minute limit on input tokens for a model",
    METRIC_LABELS,
)
MAX_MODEL_OUTPUT_TOKENS = Gauge(
    "model_max_output_tokens",
    "The per-minute limit on output tokens for a model",
    METRIC_LABELS,
)

# The counter and histogram from `instrumentators/base.py` can be removed once
# the SLIs stop using these. Then all requests, not just the code-suggestion ones
# will be instrumented
# We'll remove this as part of https://gitlab.com/gitlab-org/modelops/applied-ml/code-suggestions/ai-assist/-/issues/441
INFERENCE_COUNTER = Counter(
    "model_inferences_total",
    "The total number of inferences on a model with a label",
    INFERENCE_DETAILS,
)

INFERENCE_DURATION_S = Histogram(
    "inference_request_duration_seconds",
    "Duration of the inference request in seconds",
    INFERENCE_DETAILS,
    buckets=(0.5, 1, 2.5, 5, 10, 30, 60),
)

INFERENCE_INPUT_TOKENS = Counter(
    "inference_input_tokens",
    "The total number of input tokens processed by the model",
    INFERENCE_DETAILS,
)

INFERENCE_OUTPUT_TOKENS = Counter(
    "inference_output_tokens",
    "The total number of output tokens generated by the model",
    INFERENCE_DETAILS,
)


def _update_token_usage(model: str, usage: UsageMetadata) -> None:
    current_usage = token_usage.get()

    if current_usage is None:
        return

    current_usage.setdefault(model, {"input_tokens": 0, "output_tokens": 0})
    current_usage[model]["input_tokens"] += usage["input_tokens"]
    current_usage[model]["output_tokens"] += usage["output_tokens"]

    token_usage.set(current_usage)


logger = structlog.get_logger()


class ModelRequestInstrumentator:
    class WatchContainer:
        def __init__(
            self,
            model_provider: str,
            labels: dict[str, str],
            limits: Optional[ModelLimits],
            streaming: bool,
            unit_primitive: Optional[GitLabUnitPrimitive] = None,
            internal_event_client: Optional[InternalEventsClient] = None,
        ):
            self.model_provider = model_provider
            self.labels = labels
            self.limits = limits
            self.error = False
            self.error_type = ERROR_TYPE_NONE
            self.streaming = streaming
            self.start_time = None
            self.unit_primitive = unit_primitive
            self.internal_event_client = internal_event_client
            self.finish_reason = "unknown"

        def start(self):
            """Register the start of the inference request.

            Sets the start time to be used for duration calculations when `finish()` is called.
            """
            self.start_time = time.perf_counter()

            if self.limits:
                if "concurrency" in self.limits:
                    MAX_CONCURRENT_MODEL_INFERENCES.labels(**self.labels).set(
                        self.limits["concurrency"]
                    )
                if "input_tokens" in self.limits:
                    MAX_MODEL_INPUT_TOKENS.labels(**self.labels).set(
                        self.limits["input_tokens"]
                    )
                if "output_tokens" in self.limits:
                    MAX_MODEL_OUTPUT_TOKENS.labels(**self.labels).set(
                        self.limits["output_tokens"]
                    )

            INFERENCE_IN_FLIGHT_GAUGE.labels(**self.labels).inc()

        def register_error(self, exception: Optional[Exception] = None):
            self.error = True

            if not exception:
                self.error_type = ERROR_TYPE_OTHER
                return

            error_message = str(exception)

            # Check for prompt too long in error message (case-insensitive, works
            # across multiple providers)
            # Vertex: "Prompt is too long"
            # Anthropic: "prompt is too long: X tokens > Y maximum"
            # LiteLLM: "Prompt is too long"
            if "prompt is too long" in error_message.lower():
                self.error_type = ERROR_TYPE_PROMPT_TOO_LONG
                return

            # Check for overloaded in error message
            # LiteLLM: "Overloaded"
            if "overloaded" in error_message.lower():
                self.error_type = ERROR_TYPE_OVERLOADED
                return

            # Check if exception has a status_code or code attribute (works across
            # providers)
            # Anthropic/OpenAI use status_code, LiteLLM uses code
            status_code = getattr(exception, "status_code", None) or getattr(
                exception, "code", None
            )

            if not status_code:
                self.error_type = ERROR_TYPE_OTHER
                return

            status_code_map = {
                400: ERROR_TYPE_HTTP_400,
                403: ERROR_TYPE_PERMISSION_ERROR,
                503: ERROR_TYPE_SERVICE_UNAVAILABLE,
            }

            self.error_type = status_code_map.get(status_code, ERROR_TYPE_OTHER)

        def register_token_usage(
            self,
            model: str,
            usage: UsageMetadata,
            internal_event_extra: dict[str, Any] | None = None,
        ):
            token_usage_labels = {**self._detail_labels(), "model_name": model}

            _update_token_usage(model, usage)

            self._update_llm_operations(model, usage)
            self._track_usage(model, usage, internal_event_extra or {})

            INFERENCE_INPUT_TOKENS.labels(**token_usage_labels).inc(
                usage["input_tokens"]
            )
            INFERENCE_OUTPUT_TOKENS.labels(**token_usage_labels).inc(
                usage["output_tokens"]
            )

        def register_message(self, message: BaseMessage):
            finish_reason = message.response_metadata.get(
                "finish_reason"
            ) or message.response_metadata.get("stop_reason")

            if not finish_reason:
                return

            if str(finish_reason).lower() in LLMFinishReason:
                self.finish_reason = finish_reason
            else:
                logger.error(
                    f"Unexpected LLM response stop reason: {finish_reason}",
                    source=__name__,
                )
                self.finish_reason = "other"

        def finish(self):
            """Register the end of the inference request.

            Duration is calculated from the start time set by `start()`.
            """
            INFERENCE_IN_FLIGHT_GAUGE.labels(**self.labels).dec()

            duration = time.perf_counter() - self.start_time
            logger.info("Request to LLM complete", source=__name__, duration=duration)

            detail_labels = self._detail_labels()

            INFERENCE_COUNTER.labels(**detail_labels).inc()
            INFERENCE_DURATION_S.labels(**detail_labels).observe(duration)

        async def afinish(self):
            self.finish()

        def _track_usage(
            self, model: str, usage: UsageMetadata, internal_event_extra: dict[str, Any]
        ):
            # Access langchain usage_metadata for optional cache
            # specific token details
            input_token_details = usage.get("input_token_details", {})
            cache_creation = input_token_details.get("cache_creation", 0)
            cache_read = input_token_details.get("cache_read", 0)

            # Optional event tracking for TTL prompt caching
            ephemeral_5m_input_tokens = input_token_details.get(
                "ephemeral_5m_input_tokens", 0
            )
            ephemeral_1h_input_tokens = input_token_details.get(
                "ephemeral_1h_input_tokens", 0
            )

            token_usage_data = {
                **self.labels,
                "model_name": model,
                "model_provider": self.model_provider,
                "input_tokens": usage["input_tokens"],
                "output_tokens": usage["output_tokens"],
                "total_tokens": usage["total_tokens"],
            }

            token_cache_usage_data = {
                "cache_read": cache_read,
                "cache_creation": cache_creation,
                "ephemeral_5m_input_tokens": ephemeral_5m_input_tokens,
                "ephemeral_1h_input_tokens": ephemeral_1h_input_tokens,
            }

            logger.info(
                "LLM call finished with token usage",
                **token_usage_data,
                **token_cache_usage_data,
            )

            if self.internal_event_client and self.unit_primitive:
                additional_properties = InternalEventAdditionalProperties(
                    label="cache_details",
                    **token_cache_usage_data,
                    **internal_event_extra,
                )
                self.internal_event_client.track_event(
                    f"token_usage_{self.unit_primitive}",
                    category=__name__,
                    additional_properties=additional_properties,
                    **token_usage_data,
                )

        def _update_llm_operations(self, model: str, usage: UsageMetadata):
            current_llm_operations = llm_operations.get()

            if current_llm_operations is None:
                return

            current_llm_operations.append(
                {
                    "token_count": usage["total_tokens"],
                    "model_id": model,
                    "model_engine": self.model_provider,
                    "model_provider": self.model_provider,
                    "prompt_tokens": usage["input_tokens"],
                    "completion_tokens": usage["output_tokens"],
                }
            )

            llm_operations.set(current_llm_operations)

        def _detail_labels(self) -> dict[str, str]:
            unit_primitive = (
                self.unit_primitive.value if self.unit_primitive else "unknown"
            )
            detail_labels = {
                "error": "yes" if self.error else "no",
                "error_type": self.error_type,
                "streaming": "yes" if self.streaming else "no",
                "feature_category": current_feature_category(),
                "unit_primitive": unit_primitive,
                "finish_reason": self.finish_reason,
            }
            return {**self.labels, **detail_labels, **build_metadata_labels()}

    def __init__(
        self,
        model_engine: str,
        model_name: str,
        limits: Optional[ModelLimits],
        model_provider: str = "",
    ):
        self.labels = {"model_engine": model_engine, "model_name": model_name}
        self.limits = limits
        self.model_provider = model_provider

    @contextmanager
    def watch(self, stream=False, unit_primitive=None, internal_event_client=None):
        watcher = ModelRequestInstrumentator.WatchContainer(
            model_provider=self.model_provider,
            labels=self.labels,
            limits=self.limits,
            streaming=stream,
            unit_primitive=unit_primitive,
            internal_event_client=internal_event_client,
        )
        watcher.start()
        try:
            yield watcher
        except Exception as ex:
            log_exception(ex, self.labels)
            watcher.register_error(ex)
            watcher.finish()
            raise

        if not stream:
            watcher.finish()


# Re-export for backward compatibility during transition
# These will be removed once all imports are updated to use lib.context directly
__all__ = [
    "ModelRequestInstrumentator",
    # Re-exports from lib.context
    "METADATA_LABELS",
    "LLMFinishReason",
    "TokenUsage",
    "LlmOperations",
    "build_metadata_labels",
    "client_capabilities",
    "client_type",
    "gitlab_realm",
    "gitlab_version",
    "language_server_version",
    "token_usage",
    "llm_operations",
    "init_token_usage",
    "init_llm_operations",
    "get_token_usage",
    "get_llm_operations",
]
