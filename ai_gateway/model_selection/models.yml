# see docs/model_selection for documentation
models:
  - name: "Codestral 25.01 - Fireworks"
    provider: "Fireworks"
    gitlab_identifier: "codestral_2501_fireworks"
    description: "Fast code generation and assistance."
    cost_indicator: "$"
    max_context_tokens: 32767 # Fireworks has limited the max tokens to 32k to avoid OOM errors
    model_class_provider: litellm_completion
    family:
      - completion_fim
      - codestral
    params:
      model: "codestral-2501"
      custom_llm_provider: "fireworks_ai"
      completion_type: fim
      fim_format: "</s>[SUFFIX]{suffix}[PREFIX]{prefix}[MIDDLE]"
      temperature: 0.7
      max_tokens: 48
      max_retries: 10

  - name: "Codestral 25.08 - Vertex"
    provider: "Vertex"
    gitlab_identifier: "codestral_2508_vertex"
    description: "Fast code generation and assistance."
    cost_indicator: "$"
    max_context_tokens: 128000 # https://docs.cloud.google.com/vertex-ai/generative-ai/docs/partner-models/mistral/codestral-2
    model_class_provider: litellm_completion
    family:
      - completion_text
      - codestral
    params:
      model: "codestral-2"
      custom_llm_provider: "vertex_ai"
      completion_type: text

  - name: "Codestral 25.08 - Fireworks"
    provider: "Fireworks"
    gitlab_identifier: "codestral_2508_fireworks"
    description: "Fast code generation and assistance."
    cost_indicator: "$"
    max_context_tokens: 32767 # Fireworks has limited the max tokens to 32k to avoid OOM errors
    model_class_provider: litellm_completion
    family:
      - completion_fim
      - codestral
    params:
      model: "codestral-2508"
      custom_llm_provider: "fireworks_ai"
      completion_type: fim
      fim_format: "</s>[SUFFIX]{suffix}[PREFIX]{prefix}[MIDDLE]"
      temperature: 0.7
      max_tokens: 48
      max_retries: 10

  # Not listed in the model selection menu. But it's kept here to keep the deprecation message
  - name: "Codestral 25.01 - Vertex"
    provider: "Vertex"
    gitlab_identifier: "codestral_2501_vertex"
    description: "Fast code generation and assistance."
    cost_indicator: "$"
    max_context_tokens: 256000 # https://mistral.ai/news/codestral-2501
    model_class_provider: litellm_completion
    family:
      - completion_text
      - codestral
    params:
      model: "codestral-2501"
      custom_llm_provider: "vertex_ai"
      completion_type: text
      temperature: 0.7
      max_tokens: 64
    deprecation:
      deprecation_date: "2025-10-23"
      removal_version: "18.9" # Already removed from `unit_primitives.yml`

  - name: "Gemini 1.5 Flash - Vertex"
    provider: "Vertex"
    gitlab_identifier: "gemini_1_5_flash_vertex"
    description: "Multimodal for high-volume tasks."
    max_context_tokens: 1000000 # https://gemini-api.apidog.io/doc-965857
    # Note: "Gemini 1.5 Flash come with a 1-million-token context window, and Gemini 1.5 Pro comes with a 2-million-token context window.". Read the source for more information.
    model_class_provider: litellm
    params:
      model: "vertex_ai/gemini-1.5-flash-002"
      temperature: 0.0
      top_p: 1.0
      top_k: 1
      max_tokens: 1000

  - name: "Gemini 2.0 Flash Lite - Vertex"
    provider: "Vertex"
    gitlab_identifier: "gemini_2_0_flash_lite_vertex"
    description: "Optimized for high-volume, latency-sensitive tasks."
    cost_indicator: "$"
    max_context_tokens: 1000000 # https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash-lite
    model_class_provider: litellm
    params:
      model: "vertex_ai/gemini-2.0-flash-lite"
      temperature: 0.0
      top_p: 1.0
      top_k: 1
      max_tokens: 1000

  - name: "Gemini 2.5 Flash - Vertex"
    provider: "Vertex"
    gitlab_identifier: "gemini_2_5_flash_vertex"
    description: "Multimodal reasoning with extended context."
    cost_indicator: "$"
    max_context_tokens: 1000000 # https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash
    model_class_provider: litellm
    params:
      model: "gemini-2.5-flash"
      temperature: 0.7
      max_tokens: 4_096

  - name: "Claude Opus 4.0 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_opus_4_20250514"
    description: "Earlier generation model for coding, reasoning, and agentic workflows."
    max_context_tokens: 200000 # https://docs.claude.com/en/docs/about-claude/models/overview#legacy-models
    model_class_provider: anthropic
    params:
      model: "claude-opus-4-20250514"
      temperature: 0.0
      max_tokens: 4_096

  - name: "Claude Opus 4.1 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_opus_4_1_20250805"
    description: "Superseded by Sonnet 4.5 for coding, reasoning, and agentic workflows."
    max_context_tokens: 200000 # https://docs.claude.com/en/docs/about-claude/models/overview#latest-models-comparison
    model_class_provider: anthropic
    params:
      model: "claude-opus-4-1-20250805"
      temperature: 0.0
      max_tokens: 4_096

  - name: "Claude Opus 4.5 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_opus_4_5_20251101"
    description: "Claude's most intelligent model, combining maximum capability with practical performance."
    cost_indicator: "$$$$"
    max_context_tokens: 200000 # https://platform.claude.com/docs/en/about-claude/models/overview#latest-models-comparison
    proxy_provider: anthropic
    model_class_provider: anthropic
    family:
      - claude_4_5
    params:
      model: "claude-opus-4-5-20251101"
      temperature: 0.0
      max_tokens: 4_096

  - name: "Claude Opus 4.5 - Vertex"
    provider: "Anthropic"
    gitlab_identifier: "claude_opus_4_5_20251101_vertex"
    description: "Claude's most intelligent model, combining maximum capability with practical performance."
    cost_indicator: "$$$$"
    max_context_tokens: 200000 # https://platform.claude.com/docs/en/about-claude/models/overview#latest-models-comparison
    model_class_provider: litellm
    family:
      - claude_4_5
    params:
      model: "claude-opus-4-5@20251101"
      custom_llm_provider: "vertex_ai"
      temperature: 0.0
      max_tokens: 4_096
      max_retries: 1
    prompt_params:
      vertex_location: global

  - name: "Claude Opus 4.6 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_opus_4_6_20260205"
    description: "Anthropic's most intelligent model, combining maximum capability with practical performance."
    cost_indicator: "$$$$"
    max_context_tokens: 1_000_000
    proxy_provider: anthropic
    model_class_provider: anthropic
    family:
      - claude_4_5
    params:
      model: "claude-opus-4-6"
      temperature: 0.0
      max_tokens: 4_096

  - name: "Claude Opus 4.6 - Vertex"
    provider: "Anthropic"
    gitlab_identifier: "claude_opus_4_6_vertex"
    description: "Claude's most intelligent model, combining maximum capability with practical performance."
    cost_indicator: "$$$$"
    max_context_tokens: 200000 # https://platform.claude.com/docs/en/about-claude/models/overview#latest-models-comparison
    model_class_provider: litellm
    family:
      - claude_4_5
    params:
      model: "claude-opus-4-6"
      custom_llm_provider: "vertex_ai"
      temperature: 0.0
      max_tokens: 4_096
      max_retries: 1
      extra_headers:
        anthropic-beta: context-1m-2025-08-07
    prompt_params:
      vertex_location: global

  - name: "Claude Sonnet 4.6 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_sonnet_4_6"
    max_context_tokens: 200000 # https://docs.claude.com/en/docs/about-claude/models/overview#latest-models-comparison
    # Note: Claude Sonnet 4.6 supports a 1M token context window **only** when using the context-1m-2025-08-07 beta header
    proxy_provider: anthropic
    model_class_provider: anthropic
    family:
      - claude_4_5
    description: "Anthropic's flagship model for real-world coding, reasoning, and agentic workflows."
    cost_indicator: "$$$"
    params:
      model: "claude-sonnet-4-6"
      temperature: 0.0
      max_tokens: 4_096

  - name: "Claude Sonnet 4.5 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_sonnet_4_5_20250929"
    max_context_tokens: 200000 # https://docs.claude.com/en/docs/about-claude/models/overview#latest-models-comparison
    # Note: Claude Sonnet 4.5 supports a 1M token context window **only** when using the context-1m-2025-08-07 beta header
    proxy_provider: anthropic
    model_class_provider: anthropic
    family:
      - claude_4_5
    description: "Anthropic's flagship model for real-world coding, reasoning, and agentic workflows."
    cost_indicator: "$$$"
    params:
      model: "claude-sonnet-4-5-20250929"
      temperature: 0.0
      max_tokens: 4_096

  - name: "Claude Sonnet 4.5 - Vertex"
    provider: "Vertex"
    gitlab_identifier: "claude_sonnet_4_5_20250929_vertex"
    description: "Anthropic's flagship model for real-world coding, reasoning, and agentic workflows."
    cost_indicator: "$$$"
    max_context_tokens: 200000 # https://console.cloud.google.com/vertex-ai/publishers/anthropic/model-garden/claude-sonnet-4-5
    # Note: max context window tokens of 1M are supported in Beta
    family:
      - claude_4_5
    model_class_provider: litellm
    params:
      model: "claude-sonnet-4-5@20250929"
      custom_llm_provider: "vertex_ai"
      temperature: 0.0
      max_tokens: 4_096
      max_retries: 1
      extra_headers:
        anthropic-beta: context-1m-2025-08-07
    prompt_params:
      vertex_location: global

  - name: "Claude Sonnet 4.5 - Bedrock"
    provider: "Bedrock"
    gitlab_identifier: "claude_sonnet_4_5_20250929_bedrock"
    description: "Anthropic's flagship model for real-world coding, reasoning, and agentic workflows."
    cost_indicator: "$$$"
    max_context_tokens: 200000 # https://console.aws.amazon.com/bedrock/home#/model-catalog/serverless/anthropic.claude-sonnet-4-5-20250929-v1:0
    family:
      - claude_4_5
    model_class_provider: litellm
    params:
      model: bedrock/global.anthropic.claude-sonnet-4-5-20250929-v1:0
      custom_llm_provider: bedrock
      temperature: 0.0
      max_tokens: 4_096
      max_retries: 1

  - name: "Claude Haiku 4.5 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_haiku_4_5_20251001"
    max_context_tokens: 200000 # https://docs.claude.com/en/docs/about-claude/models/overview#latest-models-comparison
    proxy_provider: anthropic
    model_class_provider: anthropic
    family:
      - claude_4_5
    description: "Fast, cost-effective responses."
    cost_indicator: "$"
    params:
      model: "claude-haiku-4-5-20251001"
      temperature: 0.0
      max_tokens: 4_096

  - name: "Claude Haiku 4.5 - Vertex"
    provider: "Vertex"
    gitlab_identifier: "claude_haiku_4_5_20251001_vertex"
    description: "Fast, cost-effective responses."
    cost_indicator: "$"
    max_context_tokens: 200000 # https://console.cloud.google.com/vertex-ai/publishers/anthropic/model-garden/claude-haiku-4-5
    family:
      - claude_4_5
    model_class_provider: litellm
    params:
      model: "claude-haiku-4-5@20251001"
      custom_llm_provider: "vertex_ai"
      temperature: 0.0
      max_tokens: 4_096
      max_retries: 1
    prompt_params:
      vertex_location: global

  - name: "Claude Haiku 4.5 - Bedrock"
    provider: "Bedrock"
    gitlab_identifier: "claude_haiku_4_5_20251001_bedrock"
    description: "Fast, cost-effective responses."
    cost_indicator: "$"
    max_context_tokens: 200000 # https://console.aws.amazon.com/bedrock/home#/model-catalog/serverless/anthropic.claude-haiku-4-5-20251001-v1:0
    family:
      - claude_4_5
    model_class_provider: litellm
    params:
      model: bedrock/global.anthropic.claude-haiku-4-5-20251001-v1:0
      custom_llm_provider: bedrock
      temperature: 0.0
      max_tokens: 4_096
      max_retries: 1

  - name: "Claude Sonnet 4.0 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_sonnet_4_20250514"
    description: "Earlier generation model for coding, reasoning, and agentic workflows."
    cost_indicator: "$$$"
    max_context_tokens: 200000 # https://docs.claude.com/en/docs/about-claude/models/overview#legacy-models
    # Note: Claude Sonnet 4 supports a 1M token context window when using the context-1m-2025-08-07 beta header
    proxy_provider: anthropic
    model_class_provider: anthropic
    params:
      model: "claude-sonnet-4-20250514"
      temperature: 0.0
      max_tokens: 4_096

  - name: "Claude Sonnet 4.0 - Vertex"
    provider: "Vertex"
    gitlab_identifier: "claude_sonnet_4_20250514_vertex"
    description: "Earlier generation model for coding, reasoning, and agentic workflows."
    cost_indicator: "$$$"
    max_context_tokens: 200000 # https://console.cloud.google.com/vertex-ai/publishers/anthropic/model-garden/claude-sonnet-4
    model_class_provider: litellm
    params:
      model: "claude-sonnet-4@20250514"
      custom_llm_provider: "vertex_ai"
      temperature: 0.0
      max_tokens: 4_096
      max_retries: 1
      extra_headers:
        anthropic-beta: context-1m-2025-08-07
    prompt_params:
      vertex_location: global

  - name: "Claude Sonnet 4.0 - Bedrock"
    provider: "Bedrock"
    gitlab_identifier: "claude_sonnet_4_20250514_bedrock"
    description: "Earlier generation model for coding, reasoning, and agentic workflows."
    cost_indicator: "$$$"
    max_context_tokens: 200000 # https://console.aws.amazon.com/bedrock/home#/model-catalog/serverless/anthropic.claude-sonnet-4-20250514-v1:0
    model_class_provider: litellm
    params:
      model: bedrock/global.anthropic.claude-sonnet-4-20250514-v1:0
      custom_llm_provider: bedrock
      temperature: 0.0
      max_tokens: 4_096
      max_retries: 1

  - name: "Claude Sonnet 3.5 20241022 - Vertex"
    provider: "Vertex"
    cost_indicator: "$$$"
    gitlab_identifier: "vertex" # For backwards compatibility with requests asking for [prompt_id]/vertex
    max_context_tokens: 200000 # https://www.keywordsai.co/llm-library/claude-3-5-sonnet-20241022
    model_class_provider: litellm
    params:
      model: "claude-3-5-sonnet-v2@20241022"
      custom_llm_provider: "vertex_ai"
      temperature: 0.0
      max_tokens: 4_096

  - name: "Claude 3 Haiku - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_3_haiku_20240307"
    description: "Earlier generation model for high-volume tasks."
    cost_indicator: "$"
    max_context_tokens: 200000 # https://docs.claude.com/en/docs/about-claude/models/overview#legacy-models
    proxy_provider: anthropic
    model_class_provider: anthropic
    params:
      model: "claude-3-haiku-20240307"
      temperature: 0.0
      max_tokens: 4_096

  - name: "Gemini 3.1 Pro Preview - Vertex"
    provider: "Vertex"
    gitlab_identifier: "gemini_3_1_pro_preview_vertex"
    description: "Designed to deliver strong agentic capabilities (near-Pro level) at substantial speed and value."
    cost_indicator: "$" # To be confirmed
    max_context_tokens: 1000000
    model_class_provider: google_genai
    family:
      - google_genai
    params:
      model: "gemini-3.1-pro-preview"
      temperature: 1
      max_tokens: 4_096

  - name: "GPT-5.2 - OpenAI"
    provider: "OpenAI"
    gitlab_identifier: "gpt_5_2"
    description: "OpenAI's best general-purpose model, part of the GPT-5 flagship model family."
    cost_indicator: "$$$"
    max_context_tokens: 400000 # https://platform.openai.com/docs/models/gpt-5
    proxy_provider: openai
    model_class_provider: "openai"
    family:
      - gpt_5
    params:
      model: gpt-5.2-2025-12-11
      temperature: 1.0 # this model always requires temperature to be 1
      max_tokens: 4_096

  # proxy endpoint only
  - name: "Claude 2.1 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_2_1"
    description: "Legacy model, superseded by Claude 3 family."
    max_context_tokens: 200000
    proxy_provider: anthropic
    model_class_provider: anthropic
    params:
      model: "claude-2.1"
      temperature: 0.0
      max_tokens: 4_096

  # proxy endpoint only
  - name: "Claude 3.5 Sonnet 20240620 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_3_5_sonnet_20240620"
    description: "Earlier Claude 3.5 Sonnet release."
    max_context_tokens: 200000
    proxy_provider: anthropic
    model_class_provider: anthropic
    params:
      model: "claude-3-5-sonnet-20240620"
      temperature: 0.0
      max_tokens: 4_096

  # proxy endpoint only
  - name: "Claude 3.5 Sonnet 20241022 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_3_5_sonnet_20241022"
    description: "Earlier Claude 3.5 Sonnet release."
    max_context_tokens: 200000
    proxy_provider: anthropic
    model_class_provider: anthropic
    params:
      model: "claude-3-5-sonnet-20241022"
      temperature: 0.0
      max_tokens: 4_096

  - name: "GPT-5.1 - OpenAI"
    provider: "OpenAI"
    gitlab_identifier: "gpt_5"
    description: "OpenAI's flagship model for coding, reasoning, writing, and agentic tasks."
    cost_indicator: "$$"
    max_context_tokens: 400000 # https://platform.openai.com/docs/models/gpt-5
    proxy_provider: openai
    model_class_provider: "openai"
    family:
      - gpt_5
    params:
      model: gpt-5.1-2025-11-13
      temperature: 1.0 # this model always requires temperature to be 1
      max_tokens: 4_096

  - name: "GPT-5-Mini - OpenAI"
    provider: "OpenAI"
    gitlab_identifier: "gpt_5_mini"
    description: "Model for high-volume coding, reasoning, and routine workflows."
    cost_indicator: "$"
    max_context_tokens: 400000 # https://platform.openai.com/docs/models/gpt-5-mini
    proxy_provider: openai
    model_class_provider: "openai"
    family:
      - gpt_5
    params:
      model: gpt-5-mini-2025-08-07
      max_tokens: 4_096

  - name: "GPT-5-Codex - OpenAI"
    provider: "OpenAI"
    gitlab_identifier: "gpt_5_codex"
    description: "Optimized for agentic coding, refactoring, code review, and extended tasks."
    cost_indicator: "$$"
    max_context_tokens: 400000 # https://platform.openai.com/docs/models/gpt-5-codex
    proxy_provider: openai
    model_class_provider: "openai"
    family:
      - gpt_5
    params:
      model: gpt-5-codex
      verbosity: "medium"
      max_tokens: 4_096

  # proxy endpoint only
  - name: "GPT-5 - OpenAI (alias)"
    provider: "OpenAI"
    gitlab_identifier: "gpt_5_alias"
    description: "Alias for GPT-5.1, maintained for backward compatibility."
    max_context_tokens: 400000
    proxy_provider: openai
    model_class_provider: "openai"
    params:
      model: gpt-5
      max_tokens: 4_096

  # proxy endpoint only
  - name: "GPT-5.1 - OpenAI (alias)"
    provider: "OpenAI"
    gitlab_identifier: "gpt_5_1_alias"
    description: "Alias for GPT-5.1, maintained for backward compatibility."
    max_context_tokens: 400000
    proxy_provider: openai
    model_class_provider: "openai"
    params:
      model: gpt-5.1
      max_tokens: 4_096

  # proxy endpoint only
  - name: "GPT-5.1-Codex - OpenAI"
    provider: "OpenAI"
    gitlab_identifier: "gpt_5_1_codex"
    description: "Earlier GPT-5.1 Codex release for coding tasks."
    max_context_tokens: 400000
    proxy_provider: openai
    model_class_provider: "openai"
    params:
      model: gpt-5.1-codex
      max_tokens: 4_096

  - name: "GPT-5.2-Codex - OpenAI"
    provider: "OpenAI"
    gitlab_identifier: "gpt_5_2_codex"
    description: "Earlier GPT-5.2 Codex release for coding tasks."
    max_context_tokens: 400000
    proxy_provider: openai
    model_class_provider: "openai"
    params:
      model: gpt-5.2-codex
      max_tokens: 4_096

  - name: Amazon Q
    gitlab_identifier: amazon_q
    max_context_tokens: 200000 # https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/q-in-ides-chat-models.html
    model_class_provider: amazon_q
    family:
      - amazon_q
    params:
      model: amazon_q

  - name: Mistral
    gitlab_identifier: mistral
    max_context_tokens: 128000 # https://docs.mistral.ai/models/mistral-medium-3-1-25-08
    model_class_provider: litellm
    family:
      - mistral
    params:
      model: mistral
      temperature: 0.0
      max_tokens: 4_096

  - name: Mixtral
    gitlab_identifier: mixtral
    max_context_tokens: 32000 # Mixtral 8x7B: https://www.prompthub.us/models/mixtral-8x7b
    # Note: Mixtral 8x22B supports 64K context window (https://www.prompthub.us/models/mixtral-8x22b)
    model_class_provider: litellm
    family:
      - mixtral
      - mistral
    params:
      model: mixtral
      temperature: 0.0
      max_tokens: 4_096

  - name: Codestral
    gitlab_identifier: codestral
    max_context_tokens: 128000 # https://docs.mistral.ai/models/codestral-25-08
    model_class_provider: litellm
    family:
      - codestral
      - mistral
    params:
      model: codestral
      temperature: 0.0
      max_tokens: 4_096

  - name: Qwen2.5-Coder-7B
    gitlab_identifier: qwen2p5-coder-7b
    max_context_tokens: 131000
    model_class_provider: litellm
    family:
      - qwen2p5-coder-7b
    params:
      model: 'qwen2p5-coder-7b'
      custom_llm_provider: fireworks_ai
      temperature: 0.0
      max_tokens: 4_096
      max_retries: 10

  - name: Llama3
    gitlab_identifier: llama3
    max_context_tokens: 128000 # https://codesphere.com/articles/taking-advantage-of-the-long-context-of-llama-3-1-2
    model_class_provider: litellm
    family:
      - llama3
    params:
      model: llama3
      temperature: 0.0
      max_tokens: 4_096

  - name: Codellama
    gitlab_identifier: codellama
    max_context_tokens: 16000 # https://huggingface.co/blog/codellama
    model_class_provider: litellm
    family:
      - codellama
    params:
      model: codellama
      temperature: 0.0
      max_tokens: 4_096

  - name: GPT
    gitlab_identifier: gpt
    max_context_tokens: 128000
    model_class_provider: litellm
    family:
      - gpt
    params:
      model: gpt
      temperature: 0.0
      max_tokens: 4_096

  - name: Codegemma
    gitlab_identifier: codegemma
    max_context_tokens: 8000 # https://ollama.com/library/codegemma
    model_class_provider: litellm
    family:
      - codegemma
    params:
      model: codegemma
      temperature: 0.0
      max_tokens: 4_096

  - name: DeepSeekCoder
    gitlab_identifier: deepseekcoder
    max_context_tokens: 16000 # https://ollama.com/library/deepseek-coder
    model_class_provider: litellm
    family:
      - deepseekcoder
    params:
      model: deepseekcoder
      temperature: 0.0
      max_tokens: 4_096

  - name: "Self-Hosted Claude"
    gitlab_identifier: "claude_3" # For backwards compatibility with requests asking for [prompt_id]/claude_3
    max_context_tokens: 200000
    model_class_provider: litellm
    family:
      - claude_3
    params:
      model: "claude-3-5-sonnet-20241022" # This will be overridden later by the actual model used
      temperature: 0.0
      max_tokens: 4_096

  - name: "Self-Hosted General Models"
    gitlab_identifier: "general" # For backwards compatibility with requests asking for [prompt_id]/general
    max_context_tokens: 200000
    model_class_provider: litellm
    family:
      - claude_3
    params:
      model: "claude-3-5-sonnet-20241022"
      temperature: 0.0
      max_tokens: 4_096
