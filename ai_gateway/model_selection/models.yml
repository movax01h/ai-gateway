# see docs/model_selection for documentation
models:
  - name: "Codestral 25.01 - Fireworks"
    provider: "Fireworks"
    gitlab_identifier: "codestral_2501_fireworks"
    description: "Fast code generation and assistance."
    cost_indicator: "$"
    max_context_tokens: 256000 # https://mistral.ai/news/codestral-2501
    params:
      model: "codestral-2501"
      temperature: 0.7
      max_tokens: 64

  - name: "Codestral 25.01 - Vertex"
    provider: "Vertex"
    gitlab_identifier: "codestral_2501_vertex"
    description: "Fast code generation and assistance."
    cost_indicator: "$"
    max_context_tokens: 256000 # https://mistral.ai/news/codestral-2501
    params:
      model: "codestral-2501"
      custom_llm_provider: "vertex_ai"
      temperature: 0.7
      max_tokens: 64

  - name: "Gemini 1.5 Flash - Vertex"
    provider: "Vertex"
    gitlab_identifier: "gemini_1_5_flash_vertex"
    description: "Multimodal for high-volume tasks."
    max_context_tokens: 1000000 # https://gemini-api.apidog.io/doc-965857
    # Note: "Gemini 1.5 Flash come with a 1-million-token context window, and Gemini 1.5 Pro comes with a 2-million-token context window.". Read the source for more information.
    params:
      model: "vertex_ai/gemini-1.5-flash-002"
      temperature: 0.0
      top_p: 1.0
      top_k: 1
      max_tokens: 1000

  - name: "Gemini 2.0 Flash Lite - Vertex"
    provider: "Vertex"
    gitlab_identifier: "gemini_2_0_flash_lite_vertex"
    description: "Optimized for high-volume, latency-sensitive tasks."
    cost_indicator: "$"
    max_context_tokens: 1000000 # https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-0-flash-lite
    params:
      model: "vertex_ai/gemini-2.0-flash-lite"
      temperature: 0.0
      top_p: 1.0
      top_k: 1
      max_tokens: 1000

  - name: "Gemini 2.5 Flash - Vertex"
    provider: "Vertex"
    gitlab_identifier: "gemini_2_5_flash_vertex"
    description: "Multimodal reasoning with extended context."
    cost_indicator: "$"
    max_context_tokens: 1000000 # https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash
    params:
      model: "gemini-2.5-flash"
      custom_llm_provider: "vertex_ai"
      temperature: 0.7
      max_tokens: 4_096

  - name: "Claude Opus 4.0 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_opus_4_20250514"
    description: "Earlier generation model for coding, reasoning, and agentic workflows."
    max_context_tokens: 200000 # https://docs.claude.com/en/docs/about-claude/models/overview#legacy-models
    params:
      model: "claude-opus-4-20250514"
      temperature: 0.0
      max_tokens: 4_096
      model_class_provider: anthropic

  - name: "Claude Opus 4.1 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_opus_4_1_20250805"
    description: "Superseded by Sonnet 4.5 for coding, reasoning, and agentic workflows."
    max_context_tokens: 200000 # https://docs.claude.com/en/docs/about-claude/models/overview#latest-models-comparison
    params:
      model: "claude-opus-4-1-20250805"
      temperature: 0.0
      max_tokens: 4_096
      model_class_provider: anthropic

  - name: "Claude Sonnet 4.5 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_sonnet_4_5_20250929"
    max_context_tokens: 200000 # https://docs.claude.com/en/docs/about-claude/models/overview#latest-models-comparison
    # Note: Claude Sonnet 4.5 supports a 1M token context window **only** when using the context-1m-2025-08-07 beta header
    family:
      - claude_4_5
    description: "Anthropic's flagship model for real-world coding, reasoning, and agentic workflows."
    cost_indicator: "$$$"
    params:
      model: "claude-sonnet-4-5-20250929"
      temperature: 0.0
      max_tokens: 4_096
      model_class_provider: anthropic

  - name: "Claude Sonnet 4.5 - Vertex"
    provider: "Vertex"
    gitlab_identifier: "claude_sonnet_4_5_20250929_vertex"
    description: "Anthropic's flagship model for real-world coding, reasoning, and agentic workflows."
    cost_indicator: "$$$"
    max_context_tokens: 200000 # https://console.cloud.google.com/vertex-ai/publishers/anthropic/model-garden/claude-sonnet-4-5
    # Note: max context window tokens of 1M are supported in Beta
    family:
      - claude_vertex_4_5
      - vertex
      - litellm
    params:
      model: "claude-sonnet-4-5@20250929"
      custom_llm_provider: "vertex_ai"
      temperature: 0.0
      max_tokens: 4_096
      max_retries: 1
      model_class_provider: litellm
    prompt_params:
      vertex_location: global

  - name: "Claude Haiku 4.5 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_haiku_4_5_20251001"
    max_context_tokens: 200000 # https://docs.claude.com/en/docs/about-claude/models/overview#latest-models-comparison
    family:
      - claude_4_5
    description: "Fast, cost-effective responses."
    cost_indicator: "$"
    params:
      model: "claude-haiku-4-5-20251001"
      temperature: 0.0
      max_tokens: 4_096
      model_class_provider: anthropic

  - name: "Claude Haiku 4.5 20251001 - Vertex"
    provider: "Vertex"
    gitlab_identifier: "claude_haiku_4_5_20251001_vertex"
    description: "Fast, cost-effective responses."
    cost_indicator: "$"
    max_context_tokens: 200000 # https://console.cloud.google.com/vertex-ai/publishers/anthropic/model-garden/claude-haiku-4-5
    family:
      - claude_vertex_4_5
      - vertex
      - litellm
    params:
      model: "claude-haiku-4-5@20251001"
      custom_llm_provider: "vertex_ai"
      temperature: 0.0
      max_tokens: 4_096
      max_retries: 1
      model_class_provider: litellm
    prompt_params:
      vertex_location: global

  - name: "Claude Sonnet 4.0 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_sonnet_4_20250514"
    description: "Earlier generation model for coding, reasoning, and agentic workflows."
    cost_indicator: "$$$"
    max_context_tokens: 200000 # https://docs.claude.com/en/docs/about-claude/models/overview#legacy-models
    # Note: Claude Sonnet 4 supports a 1M token context window when using the context-1m-2025-08-07 beta header
    params:
      model: "claude-sonnet-4-20250514"
      temperature: 0.0
      max_tokens: 4_096
      model_class_provider: anthropic

  - name: "Claude Sonnet 4.0 - Vertex"
    provider: "Vertex"
    gitlab_identifier: "claude_sonnet_4_20250514_vertex"
    description: "Earlier generation model for coding, reasoning, and agentic workflows."
    cost_indicator: "$$$"
    max_context_tokens: 200000 # https://console.cloud.google.com/vertex-ai/publishers/anthropic/model-garden/claude-sonnet-4
    family:
      - vertex
      - litellm
    params:
      model: "claude-sonnet-4@20250514"
      custom_llm_provider: "vertex_ai"
      temperature: 0.0
      max_tokens: 4_096
      max_retries: 1
      model_class_provider: litellm
    prompt_params:
      vertex_location: global

  - name: "Claude Sonnet 3.7 - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_sonnet_3_7_20250219"
    description: "Earlier generation model for coding, reasoning, and agentic workflows."
    cost_indicator: "$$$"
    max_context_tokens: 200000 # https://docs.claude.com/en/docs/about-claude/models/overview#legacy-models
    params:
      model: "claude-3-7-sonnet-20250219"
      temperature: 0.0
      max_tokens: 4_096
      model_class_provider: anthropic
    deprecation:
      deprecation_date: "2025-10-28"
      removal_version: "18.8"

  - name: "Claude Sonnet 3.7 - Vertex"
    provider: "Vertex"
    gitlab_identifier: "claude_sonnet_3_7_20250219_vertex"
    description: "Earlier generation model for coding, reasoning, and agentic workflows."
    cost_indicator: "$$$"
    max_context_tokens: 200000 # https://console.cloud.google.com/vertex-ai/publishers/anthropic/model-garden/claude-3-7-sonnet
    family:
      - vertex
      - litellm
    params:
      model: "claude-3-7-sonnet@20250219"
      custom_llm_provider: "vertex_ai"
      temperature: 0.0
      max_tokens: 4_096
      model_class_provider: litellm
    prompt_params:
      vertex_location: global

  - name: "Claude 3.5 Haiku - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_3_5_haiku_20241022"
    description: "Earlier generation model for high volume tasks."
    cost_indicator: "$"
    max_context_tokens: 200000 # https://docs.claude.com/en/docs/about-claude/models/overview#legacy-models
    params:
      model: "claude-3-5-haiku-20241022"
      temperature: 0.0
      max_tokens: 4_096
      model_class_provider: anthropic


  - name: "Claude Sonnet 3.5 20241022 - Vertex"
    provider: "Vertex"
    cost_indicator: "$$$"
    gitlab_identifier: "vertex" # For backwards compatibility with requests asking for [prompt_id]/vertex
    max_context_tokens: 200000 # https://www.keywordsai.co/llm-library/claude-3-5-sonnet-20241022
    family:
      - vertex
      - litellm
    params:
      model: "claude-3-5-sonnet-v2@20241022"
      custom_llm_provider: "vertex_ai"
      temperature: 0.0
      max_tokens: 4_096
      model_class_provider: litellm

  - name: "Claude 3 Haiku - Anthropic"
    provider: "Anthropic"
    gitlab_identifier: "claude_3_haiku_20240307"
    description: "Earlier generation model for high-volume tasks."
    cost_indicator: "$"
    max_context_tokens: 200000 # https://docs.claude.com/en/docs/about-claude/models/overview#legacy-models
    params:
      model: "claude-3-haiku-20240307"
      temperature: 0.0
      max_tokens: 4_096
      model_class_provider: anthropic

  - name: "Claude Sonnet 3 - Anthropic" # used by troubleshoot only
    provider: "Anthropic"
    gitlab_identifier: "claude_3_sonnet_20240229"
    description: "Earlier generation model known for its computer use capability."
    cost_indicator: "$$$"
    max_context_tokens: 200000 # https://www.keywordsai.co/llm-library/claude-3-sonnet-20240229
    params:
      model: "claude-3-sonnet-20240229"
      temperature: 0.1
      max_tokens: 2_048
      model_class_provider: anthropic

  - name: "GPT-5.1 - OpenAI"
    provider: "OpenAI"
    gitlab_identifier: "gpt_5"
    description: "OpenAI's flagship model for coding, reasoning, writing, and agentic tasks."
    cost_indicator: "$$"
    max_context_tokens: 400000 # https://platform.openai.com/docs/models/gpt-5
    family:
      - gpt_5
      - litellm
    params:
      model: gpt-5.1-2025-11-13
      model_class_provider: "openai"
      temperature: 1.0 # this model always requires temperature to be 1
      max_tokens: 4_096

  - name: "GPT-5-Mini - OpenAI"
    provider: "OpenAI"
    gitlab_identifier: "gpt_5_mini"
    description: "Model for high-volume coding, reasoning, and routine workflows."
    cost_indicator: "$"
    max_context_tokens: 400000 # https://platform.openai.com/docs/models/gpt-5-mini
    family:
      - gpt_5
      - litellm
    params:
      model: gpt-5-mini-2025-08-07
      model_class_provider: "openai"
      max_tokens: 4_096

  - name: "GPT-5-Codex - OpenAI"
    provider: "OpenAI"
    gitlab_identifier: "gpt_5_codex"
    description: "Optimized for agentic coding, refactoring, code review, and extended tasks."
    cost_indicator: "$$"
    max_context_tokens: 400000 # https://platform.openai.com/docs/models/gpt-5-codex
    family:
      - gpt_5
      - litellm
    params:
      model: gpt-5-codex
      model_class_provider: "openai"
      verbosity: "medium"
      max_tokens: 4_096

  - name: Amazon Q
    gitlab_identifier: amazon_q
    max_context_tokens: 200000 # https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/q-in-ides-chat-models.html
    family:
      - amazon_q
    params:
      model: amazon_q
      model_class_provider: amazon_q

  - name: Mistral
    gitlab_identifier: mistral
    max_context_tokens: 128000 # https://docs.mistral.ai/models/mistral-medium-3-1-25-08
    family:
      - mistral
      - litellm
    params:
      model: mistral
      temperature: 0.0
      max_tokens: 4_096

  - name: Mixtral
    gitlab_identifier: mixtral
    max_context_tokens: 32000 # Mixtral 8x7B: https://www.prompthub.us/models/mixtral-8x7b
    # Note: Mixtral 8x22B supports 64K context window (https://www.prompthub.us/models/mixtral-8x22b)
    family:
      - mixtral
      - mistral
      - litellm
    params:
      model: mixtral
      temperature: 0.0
      max_tokens: 4_096

  - name: Codestral
    gitlab_identifier: codestral
    max_context_tokens: 128000 # https://docs.mistral.ai/models/codestral-25-08
    family:
      - codestral
      - mistral
      - litellm
    params:
      model: codestral
      temperature: 0.0
      max_tokens: 4_096

  - name: Llama3
    gitlab_identifier: llama3
    max_context_tokens: 128000 # https://codesphere.com/articles/taking-advantage-of-the-long-context-of-llama-3-1-2
    family:
      - llama3
      - litellm
    params:
      model: llama3
      temperature: 0.0
      max_tokens: 4_096

  - name: Codellama
    gitlab_identifier: codellama
    max_context_tokens: 16000 # https://huggingface.co/blog/codellama
    family:
      - codellama
      - litellm
    params:
      model: codellama
      temperature: 0.0
      max_tokens: 4_096

  - name: GPT
    gitlab_identifier: gpt
    max_context_tokens: 128000
    family:
      - gpt
      - litellm
    params:
      model: gpt
      temperature: 0.0
      max_tokens: 4_096

  - name: Codegemma
    gitlab_identifier: codegemma
    max_context_tokens: 8000 # https://ollama.com/library/codegemma
    family:
      - codegemma
    params:
      model: codegemma
      temperature: 0.0
      max_tokens: 4_096

  - name: DeepSeekCoder
    gitlab_identifier: deepseekcoder
    max_context_tokens: 16000 # https://ollama.com/library/deepseek-coder
    family:
      - deepseekcoder
    params:
      model: deepseekcoder
      temperature: 0.0
      max_tokens: 4_096

  - name: "Self-Hosted Claude"
    gitlab_identifier: "claude_3" # For backwards compatibility with requests asking for [prompt_id]/claude_3
    max_context_tokens: 200000
    family:
      - claude_3
      - litellm
    params:
      model: "claude-3-5-sonnet-20241022" # This will be overridden later by the actual model used
      temperature: 0.0
      max_tokens: 4_096

  - name: "Self-Hosted General Models"
    gitlab_identifier: "general" # For backwards compatibility with requests asking for [prompt_id]/general
    max_context_tokens: 200000
    family:
      - claude_3
      - litellm
    params:
      model: "claude-3-5-sonnet-20241022"
      temperature: 0.0
      max_tokens: 4_096
