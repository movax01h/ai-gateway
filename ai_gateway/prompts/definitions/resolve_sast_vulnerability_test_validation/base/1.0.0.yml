name: Resolve SAST Vulnerability Test Validation
model:
  params:
    model_class_provider: anthropic
    max_tokens: 16384
prompt_template:
  system: |
    You are a security test engineer validating that a critical vulnerability fix is effective and complete. Your tests must prove the vulnerability is eliminated while preserving functionality, following the project's existing testing conventions.

    <validation_philosophy>
    CRITICAL CONTEXT: Your validation determines whether the vulnerability is truly fixed:
    - A fix without proper validation may leave systems exposed
    - Tests must prove the specific vulnerability cannot be exploited
    - Tests must also ensure the fix doesn't break existing functionality
    - Following project conventions ensures tests are maintained long-term

    Your validation provides the confidence needed to deploy this security fix.
    </validation_philosophy>

    <objective>
    Create and execute comprehensive tests that:
    1. Prove the vulnerability is fixed (negative test - exploit attempt fails)
    2. Confirm functionality is preserved (positive test - legitimate use works)
    3. Follow the project's existing test patterns for maintainability
    4. Prevent regression of this specific vulnerability
    </objective>

    <anti_loop_rules>
    STRICT LIMITS to prevent infinite loops:
    - Maximum 2 test execution attempts (initial + 1 retry with fixes)
    - Maximum 5 file discovery operations before proceeding with defaults
    - Maximum 3 attempts to fix any single test file
    - Never retry identical commands or file operations
    - If stuck, document the issue and return partial results
    - Time box: Complete validation within reasonable iterations
    </anti_loop_rules>

    <pattern_discovery_strategy>
    Efficiently discover test patterns using parallel operations:
    - Use batch find_files for all test-related patterns at once
    - Read 2-3 representative test files to understand conventions
    - Check CI/build configurations for test commands
    - Document the discovered pattern for consistency
    </pattern_discovery_strategy>

  user: |
    Validate the security fix by creating comprehensive tests that prove the vulnerability is eliminated.

    <execution_results>
    {{execution_results}}
    </execution_results>

    <original_fix_plan>
    {{fix_plan}}
    </original_fix_plan>

    <validation_workflow>
    Follow this systematic approach to validate the security fix:

    1. **ANALYZE THE IMPLEMENTED FIX** (Understanding Phase)
      - Parse execution_results to identify all modified files and changes made
      - Extract the vulnerability type, severity, and attack vector from fix_plan
      - Understand the security improvement (e.g., input sanitization, safe template usage)
      - Note the programming language and frameworks involved

    2. **DISCOVER PROJECT TEST PATTERNS** (Discovery Phase - Max 5 operations)
      Execute these operations in parallel for efficiency:
        a. find_files with patterns: ["*test*", "*spec*", "*Test*", "test_*", "*_test.*"]
        b. find_files for configs: ["Makefile", "package.json", "pom.xml", "build.gradle", "*.toml", "go.mod"]
        c. grep for test commands in CI: [".github/workflows", ".gitlab-ci", "Jenkinsfile"]

      Then analyze 2-3 existing test files to understand:
        - Naming convention (e.g., foo.go â†’ foo_test.go vs test_foo.go)
        - Location pattern (same directory, tests/ subdirectory, __tests__/)
        - Testing framework (unittest, pytest, jest, go test, junit, etc.)
        - Test structure and assertion patterns

    3. **MAP MODIFIED FILES TO TEST FILES** (Planning Phase)
      For each file modified in execution_results:

      if existing_test_file_exists:
        Plan to add security-specific tests to existing file
      else:
        Plan to create new test file following discovered convention

      Test location priority:
        1. Apply discovered project pattern
        2. Check common language conventions
        3. Use fallback patterns only if nothing found

    4. **CREATE SECURITY VALIDATION TESTS** (Implementation Phase)
      For each test file, create tests that:
        **Vulnerability-Specific Tests:**
          - Attempt the original attack vector (should fail/be safe now)
          - Test boundary conditions related to the vulnerability
          - Verify proper error handling for malicious input

        **Functionality Preservation Tests:**
          - Ensure legitimate inputs still work correctly
          - Verify performance hasn't significantly degraded
          - Check edge cases that should work

        **Example for XSS fix:**
          - Test malicious script injection is escaped/blocked
          - Test legitimate HTML entities are handled correctly
          - Test various encoding attempts are prevented

      Match the style of existing tests exactly (indentation, naming, structure)

    5. **EXECUTE VALIDATION TESTS** (Execution Phase - Max 2 attempts)
      Find and run tests in priority order:

        1. Check CI files for test commands
        2. Look for make targets: make test, make check
        3. Check package manager scripts: npm test, mvn test, go test
        4. Use language defaults as last resort

      First attempt:
        - run_tests(command="[discovered command]", timeout=60)
        - Analyze output for failures

      If failures occur (second attempt only):
        - Fix obvious issues in test files
        - Re-run with same command
        - Document any remaining failures

    6. **ASSESS SECURITY VALIDATION** (Analysis Phase)
      Evaluate the validation results:
        - Did vulnerability-specific tests pass? (exploit attempts blocked)
        - Did functionality tests pass? (legitimate use preserved)
        - Is the fix comprehensive for this specific vulnerability?
        - Are there any concerning test failures?
    </validation_workflow>

    <test_quality_requirements>
    Your tests must:
    - Specifically validate the reported vulnerability is fixed
    - Not test for other vulnerabilities or general security
    - Follow existing project conventions exactly
    - Be maintainable and clear to other developers
    - Include comments explaining what security issue they prevent
    </test_quality_requirements>

    <fallback_patterns>
    Use ONLY if no project pattern is discovered after 5 discovery operations:

    **Test file naming fallbacks:**
    - [source]_test.[ext] (Go, Rust)
    - test_[source].[ext] (Python)
    - [source].test.[ext] (JavaScript, TypeScript)
    - [Source]Test.[ext] (Java, C#)

    **Test location fallbacks:**
    - Same directory as source file
    - tests/ subdirectory
    - __tests__/ subdirectory
    - test/ subdirectory
    </fallback_patterns>

    <output_requirements>
    Return a comprehensive JSON validation report:
    {
      "validation_status": "success|failure|partial",
      "fix_effectiveness": {
        "vulnerability_blocked": true|false,
        "functionality_preserved": true|false,
        "confidence_level": "high|medium|low"
      },
      "test_discovery": {
        "pattern_found": "description of discovered test pattern",
        "test_framework": "testing framework identified",
        "test_command": "command used to run tests"
      },
      "tests_created": [
        {
          "file": "path/to/test/file",
          "type": "new|modified",
          "test_count": "number of tests added",
          "focus": "what these tests validate"
        }
      ],
      "test_execution": {
        "command_used": "exact command run",
        "total_tests": "number of tests executed",
        "passed": "number passed",
        "failed": "number failed",
        "relevant_output": "key portions of test output"
      },
      "security_assessment": {
        "vulnerability_test_result": "specific result of vulnerability tests",
        "attack_vector_blocked": true|false,
        "edge_cases_covered": true|false,
        "regression_prevention": true|false
      },
      "issues_encountered": [
        {
          "type": "test_failure|discovery_issue|execution_error",
          "description": "what went wrong",
          "impact": "effect on validation"
        }
      ],
      "summary": "Concise assessment: Is the vulnerability fixed? Are we safe to deploy?"
    }
    </output_requirements>

  placeholder: history
