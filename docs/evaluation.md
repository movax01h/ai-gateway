# Running evaluation setup in the AIGW

Testing the AI logic is just as important as testing any other classical deterministic logic that engineers implement in Ruby, Python, etc.
When testing classical deterministic logic, we usually operate with unit tests to test components of the feature and e2e tests when testing the whole feature.
Similarly, when testing AI logic, we have the same granularity level and need to test both the full feature and its components such as prompts, the retrieval step of RAG, the ranking process of RAG, prompt guardrails, etc.

Comparing testing AI logic to classical deterministic logic, we need to mention that AI logic is non-deterministic and often requires special techniques and metrics.
For instance, a traditional unit test might verify that a function returns exactly "400" for a specific input.
In contrast, when testing an AI component, we might measure that it produces contextually appropriate responses the vast majority of the time across a test dataset.
Thus, to reflect the probabilistic nature of AI systems, machine learning engineers use the term "evaluate" rather than "test".
For consistency, we're going to continue using the term "evaluate" when referring to testing AI features throughout this document.

**Evaluation Framework at GitLab:**
We use the [Centralized Evaluation Framework (CEF)](https://gitlab.com/gitlab-org/modelops/ai-model-validation-and-research/ai-evaluation/prompt-library) and LangSmith for any sort of evaluation.
When evaluating the full AI feature is required (e2e), please feel free to rely on the [Evaluation Runner](link), which is an approach built on top of CEF to automate certain configurations.

To better match the unit-test concept described above, the AIGW now supports running evaluation pipelines for the "prompt" components.
We also plan to support evaluating RAG components and will keep updating this document with follow-up changes.

## Prompt Evaluations

Prompt Evaluation is our first step to support evaluating components of the AI features
and help engineers catch issues and errors faster before running e2e evaluation pipelines.

### Requirements

As with every AI evaluation pipeline, Prompt Evaluations require an input dataset and evaluators used to compare actual output with expected output according to some criteria/metric.

#### Datasets

The structure of the input dataset very much depends on the prompt variables (see prompt registry [documentation](aigw_prompt_registry.md) for additional details).
However, the general requirements are:

- Use LangSmith `inputs` to store prompt variables. Our evaluation scripts rely on the `inputs` to build the ready-to-use prompt. 
- Use LangSmith `outputs` to store expected outputs that will be compared to output values generated by running the prompt.

Please refer to the `dataset.generate_description.1` [dataset](https://smith.langchain.com/o/477de7ad-583e-47b6-a1c4-c4a0300e7aca/datasets/727e9927-ca44-46a1-83c0-09c59e57d081) to better explore the structure of the dataset used to evaluate 
`generate_description` prompt of the `1.0.0` version. The `generate_description` prompt contains two variables: `content` and `template`.
The dataset `inputs` contain exactly the same fields and corresponding values that will be used as replacements for the variables.
The dataset `outputs` contain only one `output` field that represents the expected prompt output value.
We don't have any strict requirements for the dataset `outputs` schema.
If your prompt returns a single value, feel free to use the `output` field to store the expected value.
If your prompt outputs a complex dictionary structure, it can also be used as a schema. 

> Note: we are currently working on an approach to automatically generate synthetic datasets for given prompts.
> Please feel free to track the progress in the [issue](https://gitlab.com/gitlab-org/modelops/ai-model-validation-and-research/ai-evaluation/prompt-library/-/issues/708).

#### Evaluators

To accelerate the development process, CEF provides a set of pre-built evaluators that are able to operate with various data schemas and structures.
The list of supported evaluators can be found in the `eval` main [script](../eval/main.py). 
At this moment, only the `correctness` evaluator is attached to prompt evaluations.
The `correctness` evaluator is schema-agnostic and relies on an LLM to assess the correctness of the actual prompt outputs by comparing them with expected outputs. 

If none of the provided evaluators work well for you, feel free to extend CEF by implementing your own evaluator and connecting it to the AIGW setup.
Here are the MRs that can be used as a reference:

- [Evaluator implementation](https://gitlab.com/gitlab-org/modelops/ai-model-validation-and-research/ai-evaluation/prompt-library/-/merge_requests/1256)
- [AIGW eval setup update](https://gitlab.com/gitlab-org/modelops/applied-ml/code-suggestions/ai-assist/-/merge_requests/2292)

### Running prompt evaluations locally

- Add a `LANGCHAIN_API_KEY` to your `.env` (see
[the eli5 prerequisites doc](https://gitlab.com/gitlab-org/modelops/ai-model-validation-and-research/ai-evaluation/prompt-library/-/tree/main/doc/eli5#prerequisites)
for instructions on how to gain access to LangSmith)
- Run `make eval` with the appropriate variables.

Example:

```shell
make eval PROMPT_ID=generate_description PROMPT_VERSION=1.0.0 DATASET=dataset.generate_description.1 EVALUATORS="correctness"
```

### Running prompt evaluations on CI

Each CI pipeline has a manual `tests:evaluation` job. You can start this job from a Merge Request pipeline to validate
prompt changes before merging them. Click on the job icon and supply the following CI variables:

- `PROMPT_ID`
- `PROMPT_VERSION`
- `DATASET`
- `EVALUATORS` (list of space-separated evaluator names, optional)

In the job output, look for the message "View the evaluation results for experiment" to get a link to the resulting
LangSmith run.

For an example, see [this CI job](https://gitlab.com/gitlab-org/modelops/applied-ml/code-suggestions/ai-assist/-/jobs/9534511015),
which points to [this LangSmith experiment](https://smith.langchain.com/o/477de7ad-583e-47b6-a1c4-c4a0300e7aca/datasets/727e9927-ca44-46a1-83c0-09c59e57d081/compare?selectedSessions=ef174a89-8d5e-403c-b80b-2f30af2d225d),
where you can see the outputs produced by the evaluation and compare them to the reference outputs in the dataset. You
can also see other metrics related to the run, like latency, token usage, and cost estimation. In the future, we'll
expand this setup to support LLM judges that can automatically evaluate the adequacy of the responses.
