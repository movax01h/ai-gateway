# Flow Registry Framework Developer Guide

The Flow Registry is a component-based framework that enables developers to build and execute AI-powered flows using
YAML configurations.
This guide provides the practical knowledge developers need to build flows effectively.

[[_TOC_]]

## Versions

Flow Registry is being versioned in order to provide stable experience, and assure backwards compatibility.
At the current moment a stable version of Flow Registry is `v1`, it should be used for all other purposes
then experimental features development

|version|description|documentation| flow config location |
|-------|-----------|-------------|---------|
| `v1`  | The current stable version of Flow Registry | [documentation](v1.md) | [v1 flow configs](/duo_workflow_service/agent_platform/v1/flows/configs/) |
| `experimental` | Development version of Flow Registry, does not offer any backwards compatibility guarantees | [documentation](experimental.md) | [experimental flow configs](/duo_workflow_service/agent_platform/experimental/flows/configs/) |

Flow Registry configs declares used version via `version` attribute.

Each of versions is being implemented by an independent Python package in AI Gateway repository, flow configs
location must adhere to a declared version

## Development Plan

Currently, we are in an ideation phase for Agent Flows. Please
see [this epic](https://gitlab.com/groups/gitlab-org/-/epics/19001) for instructions on how to submit your ideas for new
flows.

The Flow Registry is currently in active development, but the high-level plan for rollout is:

1. Add components required to replicate [existing flows](../workflows) using the Flow Registry. More information
   in [this epic](https://gitlab.com/groups/gitlab-org/duo-workflow/-/epics/1)
1. Create an AI Catalog UI to allow internal teams to build and test their own Agent Flows

In the meantime, it is possible to set up the Flow Registry locally, and create agent flows via YAML files. This guide
will explain the details of how to do that. But note that it can be quite time-intensive to do this setup and once the
Flow Registry is ready this setup will no longer be required for those building flows.

## Quick Start

For quick start guide please visit to _v1_ [documentation](v1.md#quick-start) page

## Contribution Guidelines

If you which to contribute to Flow Registry framework please familiarize yourself with [Contribution Guidelines](./contribution_guidelines.md).

## Key Framework Concepts

A **Component** is a reusable building block that performs a specific task in your flow.
Each component declares its outputs, which will be available for subsequent components to read from. In addition each
component may request inputs, that should be sourced from outputs of proceeding components, or initial flow trigger
request data. Finally some components accept additional configuration parameters like  _tools_ or _prompt_id_ and
_prompt_version_. Consult component documentation to understand how it can be used, and what configuration it requires.
Components are stateless and compose together to create complex flows.

A **Router** determines the flow control between components.
Routers are either simple (always go to the next component) or conditional (route based on some condition).
They define the execution path through your flow.

Components and routers connect through the **Input/Output System**.
Components produce outputs that are automatically stored in the flow state, and these outputs are consumed by other
components through their input configuration.
Routers also use component outputs to make routing decisions, creating dynamic flow paths based on the results of
component execution.

## YAML Configuration Structure

YAML configuration files define the structure and behavior of your flows.
Every flow YAML file must contain these top-level sections that specify components, routing logic, and execution
parameters.

```yaml
version: "v1"
environment: ambient

components:
# List of components (see Component Types section)

routers:
# Define flow between components (see examples below)

flow:
    entry_point: "component_name"  # Name of first component to run
```

### Required Fields

- **version**: Declares Flow Registry version used by a config
- **environment**: Controls level of interaction between a human and AI agents
- **components**: List of components that make up your flow
- **routers**: Define how components connect to each other
- **flow**: Specify the entry point component and other options

## Input/Output System

The Input/Output System is the core mechanism that enables data flow between components in your flow.
This system manages all data passing between components and routers through a structured shared state, ensuring seamless
communication throughout the workflow execution.

### Understanding Flow State

Components and routers communicate through a shared state with these fields:

- **context**: Nested dictionary for data exchange
- **conversation_history**: Component-specific message history
- **status**: Current workflow status

### Input Syntax

Inputs use the format `target:path.to.data`:

```yaml
inputs:
    - "context:goal"                        # Access goal from context
    - "context:previous_agent.final_answer" # Access result from previous component
    - "status"                              # Access workflow status
```

Input paths must reference existing data in the flow state.
If a component tries to access non-existent data, the flow will fail.

### Input Aliases

Input aliases provide cleaner data mapping and improved readability:

```yaml
inputs:
    - from: "context:source_data"
      as: "input_data"
    - from: "context:analyzer.final_answer"
      as: "analysis_results"
```

The `as` keyword provides these benefits:

- **Simplifying prompt templates**: Instead of referencing `{{context:analyzer.final_answer}}` in your prompt, you use
  `{{analysis_results}}`
- **Making flows more readable**: Clear, descriptive names improve flow understanding
- **Reducing coupling**: Components don't need to know internal structure of other components' outputs

### Input Literals

Input literals can be used to explicitly state values for inputs, by adding `literal: true`. When using input literals,
the `as` keyword is required:

```yaml
inputs:
    - from: "file.txt"
      as: "file_path"
      literal: true
```

This will set the value of the input variable `file_path` to be `file.txt`, rather than interpreting the input source as
a path.

### Additional Context

Additional Context can be passed to the Flow (in additional to the `goal`), but the schema for these fields must be defined in the `flow` section of the YAML file:

```yaml
flow:
  entry_point: "first_component_name"
  inputs:
    - category: merge_request_info
      input_schema:
        url:
          type: string
          format: uri
          description: GitLab Merge Request URL
        source_branch:
          type: string
          description: Merge Request Source Branch
    - category: pipeline_info
      input_schema:
        url:
          type: string
          format: uri
          description: GitLab Pipeline URL
```

The `format` and `description` fields are optional in the definitions. Allowed `type` and `format` fields can be found in the [jsonschema](https://json-schema.org/docs) docs.

When making the call to the Service API, these additional context parameters are passed in as serialized JSON in the `Content` fields:

```json
"additional_context": [
    {"Category": "merge_request_info", "Content": "{\"url\": \"www.example.com\", \"source_branch\": \"testbranch\"}"},
    {"Category": "pipeline_info", "Content": "{\"url\": \"www.example.com\"}"}
]
```

### Output

Output management handles the automatic production and storage of component results.
Each component automatically produces outputs that may be consumed by other components.
The outputs are automatically available to subsequent components when they are referenced in their `inputs`
configuration or used in router conditions.
Refer to the documentation of every component to check what outputs it produces.

## UI Logs

UI logs provide visibility into component execution progress and results for end users.
This system captures and displays component activities in the user interface, enabling real-time feedback about workflow
execution.

Each component defines its own set of available events to be logged and visualized.
For example, the AgentComponent provides events for tool execution and final responses, allowing users to see exactly
what the agent is doing and what results it produces.

You need to specify what events to log for every component that provides UI feedback.
The events you choose determine what information users see about the component's execution in the Duo interface.

## Routers

Routers control the flow execution path between components.
These components determine how workflow execution moves from one step to the next, enabling both linear and conditional
flow patterns.

### Simple Router

A simple router always routes from one component to another without any conditions:

```yaml
routers:
    - from: "analyzer"
      to: "processor"
```

This router will always send the flow from the "analyzer" component to the "processor" component.

### Conditional Router

A conditional router makes routing decisions based on component outputs or flow state:

```yaml
routers:
    - from: "decision_maker"
      condition:
          input: "context:decision_maker.final_answer"
          routes:
              "approve": "approval_handler"
              "reject": "rejection_handler"
              "default_route": "manual_review"
```

This router examines the `final_answer` output from the "decision_maker" component and routes to different components
based on the content:

- If the answer equals to "approve", route to the "approval_handler" component
- If the answer equals to "reject", route to the "rejection_handler" component
- For any other content, route to the "manual_review" component (default_route)

### Complex Multi-Path Router

```yaml
routers:
    - from: "file_analyzer"
      condition:
          input: "status"
          routes:
              "completed": "report_generator"
              "error": "error_handler"
              "partial": "manual_processor"
              "default_route": "end"
```

This router uses the workflow status to determine the next step in processing, allowing for sophisticated error handling
and partial result processing.

## Component Types

To see complete list of available components see the latest Flow Registry version documentation [page](v1.md#component-types)

## Debugging Flows

To run and debug your flow within your local GDK:

- Set up a
  local [Agent Platform](https://gitlab.com/gitlab-org/gitlab-development-kit/-/blob/main/doc/howto/duo_agent_platform.md)
  to work with Remote Flows
- Create flow, note the name and version (in the form `<flow_config_file_name_without_extension>/<version>` eg: `prototype/experimental`)
- Run this `curl` command to start your flow:

```shell
export DEFINITION="prototype/experimental"
export GOAL="create test.sh script that outputs done to stdout"
export PROJECT_ID="19"

curl -X POST \
    -H "Authorization: Bearer $GDK_API_TOKEN" \
    -H "Content-Type: application/json" \
    -d "{
        \"project_id\": \"$PROJECT_ID\",
        \"agent_privileges\": [1,2,3,4,5],
        \"goal\": \"$GOAL\",
        \"start_workflow\": true,
        \"workflow_definition\": \"$DEFINITION\",
        \"environment\": \"web\",
        \"source_branch\": \"branch-name-to-run-in\"
    }" \
    http://gdk.test:3000/api/v4/ai/duo_workflows/workflows
```

- You can check your flow execution in your rails instance, under Pipelines
- You can confirm the flow instance in your Rails DB: `Ai::DuoWorkflows::Workflow.last`
- Examine Agent Platform logs with `gdk tail duo-workflow-service`
- Trace your flow's execution
  in [Langsmith](https://docs.gitlab.com/development/ai_features/duo_chat/#use-tracing-with-langsmith)
- Alternatively, if you are running Duo Workflow Service individually using PyCharm run configuration,
  you can use [debugging flow execution using AI Agents debugger plugin](#debugging-flow-execution-using-ai-agents-debugger-plugin)
- Or view the latest flow session state snapshot via API `curl -X GET -H "X-Per-Page: 1" -H "X-Page: 1" -H "Authorization: Bearer $GDK_API_PRIVATE_TOKEN" 'https://gdk.test:3000/api/v4/ai/duo_workflows/workflows/1139603/checkpoints?page=1&per_page=1' > .debug_checkpoint`

### Debugging flow execution using AI Agents Debugger plugin

- Install [AI Agents Debugger](https://plugins.jetbrains.com/plugin/26921-ai-agents-debugger) plugin in PyCharm.
- Setup PyCharm run configuration to run Duo Workflow server.
- Run any flow that invokes Duo Workflow service server.
- When the request reaches the server, the plugin analyzes the code execution and automatically attaches itself to the
  LangGraph workflow and provides detailed insights into the LangGraph agent nodes, metadata, inputs, and outputs of
  each node.
- You can analyze time spent in each node's execution, system prompts, human input and AI response, tool calls and
  output within the IDE.
